# BCM2835 "GPU_FFT"
#
# Copyright (c) 2013, Andrew Holme.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above copyright
#       notice, this list of conditions and the following disclaimer in the
#       documentation and/or other materials provided with the distribution.
#     * Neither the name of the copyright holder nor the
#       names of its contributors may be used to endorse or promote products
#       derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

##############################################################################
# Bit-rotated write

.set PASS16_STRIDE, ((1<<STAGES)/16*8)
.set PASS32_STRIDE, ((1<<STAGES)/32*8)
.set PASS64_STRIDE, ((1<<STAGES)/64*8)

##############################################################################
# Load twiddle factors

# (MM) Optimized: interleave TMU instructions to reduce QPU stalls
.macro load_tw, stride, off, num, addr
    ;shl r2, elem_num, 3
    add t0s, addr, r2;  mov r0, addr                # re[0]
    .if num == 1
        add r0, r0, r2;       ldtmu0
        add t0s, r0, 4; 
        mov ra_tw_re+off, r4; ldtmu0
        mov rb_tw_im+off, r4
    .else
        add r0, r0, r2; mov r1, stride
        add t0s, r0, 4;                             # im[0]
        add r0, r0, r1;             ldtmu0          # <re[0]
        .rep i, num-1
            mov t0s, r0;    mov ra_tw_re+off+i, r4; # re[i+1]
                                    ldtmu0          # <im[i]
            add t0s, r0, 4; mov rb_tw_im+off+i, r4; # im[i+1]
            add r0, r0, r1;         ldtmu0          # <re[i+1]
        .endr
        mov ra_tw_re+off+num-1, r4; ldtmu0          # <im[num-1]
        mov rb_tw_im+off+num-1, r4;
    .endif
.endm

##############################################################################
# VPM pointers

# (MM) Optimized: reduced VPM registers to 1
.macro inst_vpm, in_inst, out_vpm
    # (MM) Optimized: instruction packing
    .assert vpm_setup(1, 1, v32(0,2)) - vpm_setup(1, 1, v32(0,0)) == 2
    ;v8adds r1, in_inst, in_inst
    mov r0, vpm_setup(1, 1, v32( 0,0))
    add out_vpm, r0, r1;
.endm

##############################################################################

# (MM) no longer used
.macro proc, rx_ptr, label
    brr rx_ptr, label
    nop
    nop
    nop
.endm

# (MM) Optimized: clone instructions of branch target instead of nops.
.macro brr_opt, dest, target, back
    .assert back >= 0 && back <= 3
    .back back
    brr dest, target + 8 * (3 - back)
    .endb
    .clone target, (3-back)
.endm

##############################################################################

.macro write_vpm_16
    # (MM) Optimized: allow rb register for vpm setup
    ;mov vw_setup, rx_vpm
    mov vpm, r0
    mov vpm, r1;
.endm

.macro write_vpm_32
    # (MM) Optimized: no need for 2 VPM registers anymore, less memory I/O
    # allow rb register for vpm setup
    mov vw_setup, rx_vpm;   mov r2, rx_vpm
    # (MM) Optimized: less memory I/O and prepare for better instruction packing
    fadd vpm, ra_32_re, r0
    fadd vpm, rb_32_im, r1; mov r3, rb_32_im
    add vw_setup, r2, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0))
    fsub vpm, ra_32_re, r0
    fsub vpm, r3,       r1;
.endm

.macro write_vpm_64
    # (MM) Optimized: no need for 4 VPM registers anymore
    .if isRegfileB(rx_vpm)
    mov vw_setup, rx_vpm; mov r0, rx_vpm
    .else
    add r0, rx_vpm, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)); mov vw_setup, rx_vpm
    .endif
    # (MM) Optimized: less memory I/O and prepare for better instruction packing
    fadd vpm, ra_64+0, rb_64+0
    fadd vpm, ra_64+1, rb_64+1; mov r1, rb_64+1 
    .if isRegfileB(rx_vpm)
    add r0, r0, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0))
    .endif
    add r0, r0, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)); mov vw_setup, r0
    fadd vpm, ra_64+2, rb_64+2; mov r2, rb_64+2
    fadd vpm, ra_64+3, rb_64+3; mov r3, rb_64+3
    add r0, r0, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)); mov vw_setup, r0
    fsub vpm, ra_64+0, rb_64+0
    fsub vpm, ra_64+1, r1
    mov vw_setup, r0
    fsub vpm, ra_64+2, r2
    fsub vpm, ra_64+3, r3;
.endm

##############################################################################

# (MM) Unified vpm register names
.macro body_ra_save_16, arg_vdw
    # (MM) Optimized: move write_vpm_16 to body_pass_16

    mov -, vw_wait

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slave
        mov -, srel(i+1) # Master releases slave
    .endr

    bra -, ra_link_1

    # (MM) Optimized: less memory I/O, no need for rb_0x40
    mov r0, 0x40; mov vw_setup, arg_vdw
    mov vw_setup, vdw_setup_1(0) + PASS16_STRIDE-16*4
    add ra_save_ptr, ra_save_ptr, r0; mov vw_addr, ra_save_ptr
.endm

##############################################################################

# (MM) Unified vpm register names
.macro body_rx_save_slave
    # (MM) Optimized: move write_vpm_16 to body_pass_16
    # => now the same as save_slave_32

    # (MM) Optimized: easier procedure chains
    brr -, r:sync, ra_sync
    mov vr_setup, rx_vpm
    nop
    mov -, vpm
.endm

##############################################################################

.macro body_ra_save_32
    # (MM) Optimized: move write_vpm_32 to body_pass_32

    mov -, vw_wait

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slave
        mov -, srel(i+1) # Master releases slave
    .endr

    bra -, ra_link_1

    # (MM) Optimized: less memory I/O, no need for rb_0x40
    mov r0, 0x40; mov vw_setup, ra_vdw_32
    mov vw_setup, vdw_setup_1(0) + PASS32_STRIDE-16*4
    add ra_save_ptr, ra_save_ptr, r0; mov vw_addr, ra_save_ptr
.endm

##############################################################################

# (MM) no longer used
.macro body_rx_save_slave_32
    # (MM) Optimized: move write_vpm_32 to body_pass_32
    # => makes body_rx_save_slave_32 meaningless because identical to save_slave_16

    # (MM) Optimized: easier procedure chains
    brr -, r:sync, ra_sync
    mov vr_setup, rx_vpm
    nop
    mov -, vpm
.endm

##############################################################################

.macro body_ra_save_64
    mov -, vw_wait

    .rep i, 7
        mov -, srel(i+1) # Master releases slaves
    .endr

    write_vpm_64

    # (MM) Optimized: less memory I/O, no need for rb_0x40
    ;mov r0, 0x40 # joins with write_vpm

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slaves
    .endr

    mov vw_setup, vdw_setup_0(64, 16, dma_h32(0,0))
    mov vw_setup, vdw_setup_1(PASS64_STRIDE-16*4)
    add ra_save_ptr, ra_save_ptr, r0; mov vw_addr, ra_save_ptr

    .back 3
    bra -, ra_link_1
    .endb
.endm

##############################################################################

.macro body_rx_save_slave_64
    # (MM) Optimized:
    # - Calculate entry point at setup like for body_rx_sync_slave,
    # saves 7 instructions
    # - Scheduled code of write_vpm_64 with branch instructions,
    # saves another 5 instructions.
    # - Removed unnecessary code for semaphore 0 (8 instructions dead code).
    # - Furthermore store the return address as offset for the second branch to srel.
    .rep i, 7
        brr ra_temp, r:2f
        mov -, sacq(i+1) # Slave waits for master

        # start with write_vpm_64 here
        # (MM) Optimized: no need for 4 VPM registers anymore, less memory I/O
        .if isRegfileB(rx_vpm)
        mov vw_setup, rx_vpm;   mov r0, rx_vpm
        .else
        add r0, rx_vpm, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)); mov vw_setup, rx_vpm
        .endif
        fadd vpm, ra_64+0, rb_64+0
    .endr
:2
    # write_vpm_64
    # (MM) Optimized: less memory I/O and prepare for better instruction packing
    fadd vpm, ra_64+1, rb_64+1; mov r1, rb_64+1

    .if isRegfileB(rx_vpm)
    add r0, r0, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0))
    .endif
    add r0, r0, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)); mov vw_setup, r0
    fadd vpm, ra_64+2, rb_64+2; mov r2, rb_64+2
    fadd vpm, ra_64+3, rb_64+3; mov r3, rb_64+3

    add r0, r0, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)); mov vw_setup, r0
    fsub vpm, ra_64+0, rb_64+0
    fsub vpm, ra_64+1, r1
    mov vw_setup, r0
    fsub vpm, ra_64+2, r2
    fsub vpm, ra_64+3, r3

    .back 3
       # (MM) Optimized: Take the return address from above as base of the branch target.
       bra -, ra_temp, :3f - :2 - 4*8
    .endb

    .rep i, 7
        bra -, ra_link_1
        mov vr_setup, rx_vpm
        mov -, vpm
        mov -, srel(i+9) # Slave releases master
    .endr
:3
.endm

##############################################################################

.macro body_ra_sync
    mov -, vw_wait

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slave
    .if i==5
        bra -, ra_link_1
    .endif
        mov -, srel(i+1) # Master releases slave
    .endr
.endm

##############################################################################

.macro body_rx_sync_slave
    .rep i, 7
        bra -, ra_link_1
        mov -, srel(i+9) # Slave releases master
        # (MM) Optimized: wait as late as possible
        nop
        mov -, sacq(i+1) # Slave waits for master
    .endr
.endm

##############################################################################

.macro fft_twiddles_32
    # (MM) Optimized: less memory I/O
    ;                fmul r3, r0, ra_tw_re+TW32_ACTIVE # rr
    mov r2, r0;      fmul r0, r1, rb_tw_im+TW32_ACTIVE # ii
    fsub r0, r3, r0; fmul r3, r1, ra_tw_re+TW32_ACTIVE # ir
                     fmul r1, r2, rb_tw_im+TW32_ACTIVE;# ri
    fadd r1, r1, r3;
.endm

##############################################################################
# FFT-16 codelet

# (MM) Optimized: joined load_xxx and ldtmu in FFT-16 codelet
# Extracted stride from load_xxx to make it part of the procedure.
# Saves several instructions and delays ldtmu for linear reads.

# redefine this to compile a fixed stride into the fft_16 bodies.
.set DEF_STRIDE, 0

.macro body_fft_16_rev
    ldtmu0
    mov r0, r4; ldtmu0
    interleave r0, r4
# (MM) Optimized: better use of both ALUs, no need for ra_temp
                               fmul      r2, ra_tw_re+TW16_ACTIVE+0, r0
                               fmul      r3, rb_tw_im+TW16_ACTIVE+0, r1
    .back 3
    brr -, r:fft_16_cont
    .endb
.endm

.macro body_fft_16_lin
    and.setf -, elem_num, 1<<0
    ldtmu0
    # (MM) Optimized: better use of both ALUs, no need for ra_temp
    mov r0, r4;                fmul      r2, ra_tw_re+TW16_ACTIVE+0, r4; ldtmu0
    mov r1, r4;                fmul      r3, rb_tw_im+TW16_ACTIVE+0, r4
:fft_16_cont
    .rep i, 3
        fsub.ifnz r0, r2, r3;      fmul.ifnz r2, rb_tw_im+TW16_ACTIVE+i, r0
        mov r3, 1<<(i+1);          fmul.ifnz r1, ra_tw_re+TW16_ACTIVE+i, r1
        fadd.ifnz r1, r1, r2;      mov.ifz  r2, r0 << (1<<i)
        fadd.ifz  r0, r2, r0;      mov.ifnz r2, r0 >> (1<<i)
        fsub.ifnz r0, r2, r0;      mov.ifz  r2, r1 << (1<<i)
        fadd.ifz  r1, r2, r1;      mov.ifnz r2, r1 >> (1<<i)
        fsub.ifnz r1, r2, r1;      fmul      r2, ra_tw_re+TW16_ACTIVE+i+1, r0
        and.setf -, elem_num, r3;  fmul      r3, rb_tw_im+TW16_ACTIVE+i+1, r1
    .endr
    # (MM) rotation by 8 left and right is identical => no if required
    # and use -(0-r1) instead of +r1 to make the last instruction unconditional
    # Saves another instruction.
    fsub.ifnz r0, r2, r3;      fmul.ifnz r2, rb_tw_im+TW16_ACTIVE+3, r0
    fsub.ifz  r3, 0., r1;      fmul.ifnz r1, ra_tw_re+TW16_ACTIVE+3, r1
    fadd.ifnz r1, r1, r2;      mov       r2, r0 << 8
    fadd.ifz  r0, r2, r0;      mov.ifnz  r3, r1
    fsub.ifnz r0, r2, r0;      mov       r2, r1 << 8
    fsub      r1, r2, r3;
    # (MM) Optimized: easier procedure chains
    # place branch from outside individually
.endm

.macro bodies_fft_16
:load_fft_16_rev
    read_rev DEF_STRIDE

#:fft_16_rev
    body_fft_16_rev

:load_fft_16_lin
    read_lin DEF_STRIDE

#:fft_16_lin
    body_fft_16_lin
.endm

##############################################################################

.macro bit_rev, shift, mask
    # (MM) Optimized: use MUL ALU for small shifts
    and r1, r0, mask
    .if shift == 1
        shr r0, r0, shift
        and r0, r0, mask;  v8adds r1, r1, r1 # can't overflow because of mask
    .elseif shift == 2
        shr r0, r0, shift; v8adds r1, r1, r1 # can't overflow because of mask
        and r0, r0, mask;  v8adds r1, r1, r1 # can't overflow because of mask
    .else
        shr r0, r0, shift
        and r0, r0, mask
        shl r1, r1, shift
    .endif
    # (MM) Optimized: mv8adds can be combined easier; cannot overflow
    mv8adds  r0, r0, r1;
.endm

##############################################################################

# (MM) Optimizes: allow alternate source registers
# and allow instruction combining if re is not r0
.macro interleave, re, im
    .if im != r1
    ;and.setf -, elem_num, 1
    mov.ifnz r1, im;         mov.ifz  r1, re << 1
    mov.ifz  r0, re;         mov.ifnz r0, im >> 1
    .else
    and.setf -, elem_num, 1; mov r2, re
    mov.ifz  r0, r2;         mov.ifnz r0, im >> 1
    mov.ifnz r1, im;         mov.ifz  r1, r2 << 1
    .endif
.endm

##############################################################################

.macro read_rev, stride
    # (MM) Optimized bit swap using regfile A unpack
    # Saves a B register and 3 instruction but no speed.
    shl.unpack8a r0, ra_load_idx, 8
    or.unpack8b r0, ra_load_idx, r0

    bit_rev 1, rx_0x5555    # 16 SIMD
.if stride != 0
    # (MM) Optimized: join stride with v8adds
    ;add ra_load_idx, ra_load_idx, stride
.endif
    bit_rev 2, rx_0x3333
    bit_rev 4, rx_0x0F0F
    ;mov r3, 4 # prepare for below

.if STAGES>13               # reversal creates left shift by 16-STAGES
    shl r0, r0, STAGES-13
.endif
.if STAGES<13
    shr r0, r0, 13-STAGES
.endif
    mv8adds r2, r0, r3; # r3 = 4

    # (MM) Optimizes: allow alternate source registers for interleave
    interleave r0, r2
    swizzle

    add t0s, ra_addr_x, r0  # r0 = re = {idx[0:STAGES-1], 1'b0, 2'b0}
    add t0s, ra_addr_x, r1  # r1 = im = {idx[0:STAGES-1], 1'b1, 2'b0}
.endm

# (MM) Optimized: extracted stride from load_xxx to make read part of a procedure
.macro load_rev
    read_rev 0
    # (MM) Optimized: joined ldtmu in FFT-16 codelet
    .back 3
    brr ra_link_0, r:fft_16_rev
    .endb
.endm

##############################################################################

.macro read_lin, stride
    shl r0, ra_load_idx, 3; mov r1, ra_load_idx
    # (MM) Optimized: write t0s earlier
    add t0s, r0, ra_addr_x; v8adds r2, ra_addr_x, 4 # can't overflow, LSBs of ra_addr_x are 0
.if stride != 0
    add ra_load_idx, r1, stride
.endif
    add t0s, r0, r2;
.endm

# (MM) Optimized: extracted stride from load_xxx to make read part of a procedure
.macro load_lin
    read_lin 0
    # (MM) Optimized: joined ldtmu in FFT-16 codelet
    .back 3
    brr ra_link_0, r:fft_16_lin
    .endb
.endm

##############################################################################
# Unpack twiddles

.macro unpack_twiddles
# (MM) Optimized: do not rotate r2/r3 unnecessarily at the last turn
.rep i, 3
    and.setf -, elem_num, (8>>i)
    mov ra_tw_re+TW16_ACTIVE+3-i, r2; mov.ifnz r2, r2 >> (8>>i)
    mov rb_tw_im+TW16_ACTIVE+3-i, r3; mov.ifnz r3, r3 >> (8>>i)
.endr
    mov ra_tw_re+TW16_ACTIVE+3-3, r2; mov rb_tw_im+TW16_ACTIVE+3-3, r3
.endm

##############################################################################
# Rotate twiddles

.macro next_twiddles_32, tw32
    # (MM) Optimized: better use of both ALUs
    mov r2, ra_tw_re+TW32_ACTIVE; fmul r0, ra_tw_re+TW32_ACTIVE, rb_tw_im+tw32 # b.cos
    mov r3, rb_tw_im+TW32_ACTIVE; fmul r1, rb_tw_im+TW32_ACTIVE, ra_tw_re+tw32 # a.sin
    fsub r1, r1, r0;                                     fmul r0, r3, rb_tw_im+tw32 # b.sin
    fsub rb_tw_im+TW32_ACTIVE, rb_tw_im+TW32_ACTIVE, r1; fmul r1, r2, ra_tw_re+tw32 # a.cos
    fadd r1, r1, r0
    fsub ra_tw_re+TW32_ACTIVE, ra_tw_re+TW32_ACTIVE, r1;
.endm

.macro next_twiddles_16, tw16
    # (MM) Optimized: better use of both ALUs
    mov r2, ra_tw_re+TW16_ACTIVE+3; fmul r1, ra_tw_re+TW16_ACTIVE+3, rb_tw_im+tw16 # b.cos
    mov r3, rb_tw_im+TW16_ACTIVE+3; fmul r0, rb_tw_im+TW16_ACTIVE+3, ra_tw_re+tw16 # a.sin
    fsub r3, r0, r1;                     fmul r1, r3, rb_tw_im+tw16 # b.sin
    fsub r3, rb_tw_im+TW16_ACTIVE+3, r3; fmul r0, r2, ra_tw_re+tw16 # a.cos
    fadd r2, r0, r1
    fsub r2, ra_tw_re+TW16_ACTIVE+3, r2

    unpack_twiddles
.endm

##############################################################################
# Alternate input/output buffers between stages

.macro swap_buffers
    mov rb_addr_y, ra_addr_x; mov ra_addr_x, rb_addr_y
.endm

##############################################################################
# Reset counters and twiddles

.macro init_stage_32, tw32
    mov ra_tw_re+TW32_ACTIVE, ra_tw_re+tw32; mov rb_tw_im+TW32_ACTIVE, rb_tw_im+tw32
.endm

.macro init_stage_16, tw16, m
    # (MM) Optimized: better use of both ALUs
    .if isRegfileB(rx_inst)
    ;mov r0, rx_inst
    shl r0, r0, m
    .else
    ;shl r0, rx_inst, m
    .endif
    add ra_load_idx, r0, elem_num; mov r3, rb_tw_im+tw16
    mov ra_save_ptr, rb_addr_y;    mov r2, ra_tw_re+tw16
    unpack_twiddles
.endm

##############################################################################

.set LOAD_STRAIGHT, 0
.set LOAD_REVERSED, 1

.macro loader_16, stride, mode, back
    # (MM) Optimized: extracted stride from load_xxx to make read part of a procedure
    # (MM) Optimized: clone instructions of branch target instead of nops.
    .if mode==LOAD_REVERSED
        brr_opt ra_link_0, r:load_fft_16_rev, back
    .else
        brr_opt ra_link_0, r:load_fft_16_lin, back
    .endif

    add ra_load_idx, ra_load_idx, stride;
.endm

.macro body_pass_16, mode
    loader_16 rb_0x80, mode, 0

    # (MM) Optimized: use xor for vpm swap (less memory I/O)
    # and save a A register and a B register
    .assert vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)) == 16
    .assert vdw_setup_0(32, 16, dma_h32(16,0)) - vdw_setup_0(32, 16, dma_h32(0,0)) == 0x800
    ;mov r3, rb_0x80
    mul24 r2, 4, 4;                                  # r2 = 16

    # (MM) Optimized: move write_vpm_16 to body_pass_16
    # and expand inline to pack with VPM swap code, saves 3 instructions
    xor vw_setup,  rx_vpm, r2;     mul24 r3, r2, r3  # r3 = 0x800
    xor ra_vdw_16, ra_vdw_16, r3;  mov vpm, r0
    xor rx_vpm,    rx_vpm, r2;     mov vpm, r1;

    # (MM) Optimized: easier procedure chains
    # place branch from outside individually
.endm

.macro body_pass_32, mode
    loader_16 rb_0xF0, mode, 0
    mov ra_32_re, r0; mov rb_32_im, r1
    loader_16 0x10, mode, 2
    fft_twiddles_32

    # (MM) Optimized: no need for 2 VPM register sets anymore
    # (MM) Optimized: use xor for vpm swap (less memory I/O)
    # and save a A register and a B register
    .assert vpm_setup(1, 1, v32(32,0)) - vpm_setup(1, 1, v32(0,0)) == 32
    .assert vdw_setup_0(32, 16, dma_h32(32,0)) - vdw_setup_0(32, 16, dma_h32(0,0)) == 0x1000
    ;mov r2, 16
    shl r3, r2, 8;                 v8adds r2, r2, r2  # r3 = 0x1000; r2 = 32
    xor r2, rx_vpm, r2

    # (MM) Optimized: move write_vpm_32 to body_pass_32
    # and expand inline to join with VPM swap code
    xor ra_vdw_32, ra_vdw_32, r3;  mov vw_setup, r2
    fadd vpm, ra_32_re, r0;        mov rx_vpm, r2
    # (MM) Optimized: less memory I/O and prepare for better instruction packing
    fadd vpm, rb_32_im, r1;        mov r3, rb_32_im
    add vw_setup, r2, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0))
    fsub vpm, ra_32_re, r0
    fsub vpm, r3,       r1;

    # (MM) Optimized: easier procedure chains
    # place branch from outside individually
.endm

.macro body_pass_64, mode, step
    loader_16 0x10, mode, 0
    mov ra_32_re, r0; mov rb_32_im, r1
    loader_16 0x10, mode, 2
    fft_twiddles_32

    # (MM) Optimized: less memory I/O
    fadd ra_64+0, ra_32_re, r0; mov r2, ra_32_re
    fsub ra_64+2, r2, r0
    fadd ra_64+1, rb_32_im, r1; mov r2, rb_32_im
    fsub ra_64+3, r2, r1;

    loader_16 step, mode, 3
    mov ra_32_re, r0; mov rb_32_im, r1
    loader_16 0x10, mode, 2
    fft_twiddles_32

    # (MM) Optimized: better use of both ALUs and accumulators
    fadd r3, rb_32_im, r1
    fsub rb_32_im, rb_32_im, r1; fmul r1, r3, ra_tw_re+TW64_P1_BASE0 # ir
    fadd r2, ra_32_re, r0;       fmul r3, r3, rb_tw_im+TW64_P1_BASE0 # ii
    fsub ra_32_re, ra_32_re, r0; fmul r0, r2, rb_tw_im+TW64_P1_BASE0 # ri
    fadd rb_64+1, r0, r1;        fmul r2, r2, ra_tw_re+TW64_P1_BASE0 # rr
    fsub rb_64+0, r2, r3;        fmul r0, ra_32_re, rb_tw_im+TW64_P1_BASE1 # ri
    mov r3, rb_32_im;            fmul r1, rb_32_im, ra_tw_re+TW64_P1_BASE1 # ir
    # (MM) Optimized: easier procedure chains
    # place branch from outside individually
    mov r2, ra_32_re;            fmul r3, r3, rb_tw_im+TW64_P1_BASE1 # ii
    fadd rb_64+3, r0, r1;        fmul r2, r2, ra_tw_re+TW64_P1_BASE1 # rr
    fsub rb_64+2, r2, r3;
.endm

##############################################################################

.macro exit, flag
    # (MM) Optimized: no need to wait with thrend
    mov interrupt, flag; thrend
    nop
    nop
.endm

##############################################################################
