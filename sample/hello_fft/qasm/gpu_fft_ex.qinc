# BCM2835 "GPU_FFT" release 2.0 BETA
#
# Copyright (c) 2014, Andrew Holme.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above copyright
#       notice, this list of conditions and the following disclaimer in the
#       documentation and/or other materials provided with the distribution.
#     * Neither the name of the copyright holder nor the
#       names of its contributors may be used to endorse or promote products
#       derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

##############################################################################
# Macro baseline

.include "gpu_fft.qinc"

##############################################################################
# Redefining some macros

.if STAGES>16
.macro read_rev, stride
    # (MM) Optimized bit swap using regfile A unpack and swap only 24 bit
    # saves 2 rb and a bunch of cycles
    ;mov.unpack8c r0, ra_load_idx
    shl.unpack8b r1, ra_load_idx, 8
    shl.unpack8a r1, ra_load_idx, -16; v8adds r0, r0, r1
    mv8adds r0, r0, r1;
.if stride != 0
    # (MM) Optimized: join stride with v8adds
    ;add ra_load_idx, ra_load_idx, stride
.endif

    bit_rev 1,  rx_0x55555555
    bit_rev 2,  rx_0x33333333
    bit_rev 4,  rx_0x0F0F0F0F
    ;mov r2, 4 # prepare for below

.if STAGES>21
    shl r0, r0, STAGES-21
.endif
.if STAGES<21
    shr r0, r0, 21-STAGES
.endif
    mv8adds r2, r0, r2; # r2 = 4

    interleave r0, r2

    add t0s, ra_addr_x, r0 # r0 = re = {idx[0:STAGES-1], 1'b0, 2'b0}
    add t0s, ra_addr_x, r1 # r1 = im = {idx[0:STAGES-1], 1'b1, 2'b0}
.endm
.endif

.if STAGES>17
.macro body_ra_save_16, arg_vdw
    # (MM) Optimized: move write_vpm_16 to body_pass_16

    # (MM) Optimized: prepare before wait
    mov r2, 0x40;              mov r1, ra_save_ptr
    add ra_save_ptr, r1, r2;   mul24 r2, r2, (vdw_setup_0(1, 16, dma_h32(1,0)) - vdw_setup_0(1, 16, dma_h32(0,0))) / 0x40
    mov r3, PASS16_STRIDE

    # (MM) Optimized: less memory I/O, no need for rb_0x40, better use of both ALUs
    mov -, vw_wait;            mov r0, arg_vdw

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slave
        mov -, srel(i+1) # Master releases slave
    .endr

    .rep i, 16
        add r0, r0, r2; mov vw_setup, r0
        add r1, r1, r3; mov vw_addr,  r1
    .endr

    # (MM) Optimized: scheduling of branch instruction
    .back 3
    bra -, ra_link_1
    .endb
.endm
.endif

.if STAGES>18
.macro body_ra_save_32
    # (MM) Optimized: move write_vpm_32 to body_pass_32

    # (MM) Optimized: less memory I/O, no need for rb_0x40, better use of both ALUs
    mov -, vw_wait;            mov r0, ra_vdw_32

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slave
        mov -, srel(i+1) # Master releases slave
    .endr

    mov r2, 0x40;              mov r1, ra_save_ptr
    add ra_save_ptr, r1, r2;   mov r3, 14
    mov r2, vdw_setup_0(1, 16, dma_h32(1,0)) - vdw_setup_0(1, 16, dma_h32(0,0)); mov ra_temp, r3
    mov.setf r3, PASS32_STRIDE

    # (MM) Optimized: shorter code, slightly faster because of cache efficiency
:0  .rep i, 2
        add r0, r0, r2; mov vw_setup, r0
        add r1, r1, r3; mov vw_addr,  r1
    .endr
    .back 2
        brr.allnz -, r:0
        sub.setf ra_temp, ra_temp, 1
    .endb
    .rep i, 2
        add r0, r0, r2; mov vw_setup, r0
        add r1, r1, r3; mov vw_addr,  r1
    .endr
    # (MM) Optimized: scheduling of branch instruction
    .back 3
        bra -, ra_link_1
    .endb
.endm
.endif
